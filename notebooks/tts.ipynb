{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set of experiments for the TTS task with given voice and task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torchaudio\n",
    "from speechbrain.inference.speaker import EncoderClassifier\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = 'fFLVyWBDTfo.wav'\n",
    "OUTPUT_PATH = '../data/audios'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-xvect-voxceleb\", savedir=\"../pretrained_models/spkrec-xvect-voxceleb\")\n",
    "# audio, sr = torchaudio.load(f'{OUTPUT_PATH}/{FILENAME}')\n",
    "# embeddings = classifier.encode_batch(audio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt(f\"{OUTPUT_PATH}/{'.'.join(FILENAME.split('.')[:-1])}_embeddings.txt\", embeddings.squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthesiser = pipeline(\"text-to-speech\", \"microsoft/speecht5_tts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "# embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\")\n",
    "# speaker_embedding_sample = torch.tensor(embeddings_dataset[12][\"xvector\"]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speaker_embedding_sample.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speaker_embedding = embeddings[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speaker_embedding.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech = synthesiser(\"Hello, my dog is cooler than you!\", forward_params={\"speaker_embeddings\": speaker_embedding_sample})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sf.write(\"speech.wav\", speech[\"audio\"], samplerate=speech[\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy\n",
    "import argparse\n",
    "import torchaudio\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "spk_model = {\n",
    "    \"speechbrain/spkrec-xvect-voxceleb\": 512, \n",
    "    \"speechbrain/spkrec-ecapa-voxceleb\": 192,\n",
    "}\n",
    "\n",
    "def f2embed(wav_file, classifier, size_embed):\n",
    "    signal, fs = torchaudio.load(wav_file)\n",
    "    if fs != 16000:\n",
    "        signal = torchaudio.transforms.Resample(fs, 16000)(signal)\n",
    "    if signal.size()[0] != 1:\n",
    "        signal = signal[0].unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        embeddings = classifier.encode_batch(signal)\n",
    "        embeddings = F.normalize(embeddings, dim=2)\n",
    "        embeddings = embeddings.squeeze().cpu().numpy()\n",
    "    assert embeddings.shape[0] == size_embed, embeddings.shape[0]\n",
    "    return embeddings\n",
    "\n",
    "def process(filepath, model):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    classifier = EncoderClassifier.from_hparams(source=model, run_opts={\"device\": device}, savedir=os.path.join('/tmp', '.'.join(filepath.split('.')[:-1])))\n",
    "    size_embed = spk_model[model]\n",
    "    utt_emb = f2embed(filepath, classifier, size_embed)\n",
    "    return utt_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = process(f'{OUTPUT_PATH}/{FILENAME}', \"speechbrain/spkrec-xvect-voxceleb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings = process(f'E:/_UNIVER/UCU/1 sem/Linear algebra/Project/speaker-verification-project/data/wav/id10004/IEV-I9f21ns/00001.wav', \"speechbrain/spkrec-xvect-voxceleb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speaker_embeddings = torch.tensor(embeddings).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech = synthesiser(\"\"\"\n",
    "# Few things I wish I said (I'll add items here as they come up):\n",
    "# - The dreams and hallucinations do not get fixed with finetuning. Finetuning just \"directs\" the dreams into \"helpful assistant dreams\". \n",
    "# \"\"\", forward_params={\"speaker_embeddings\": speaker_embeddings})\n",
    "# sf.write(\"speech.wav\", speech[\"audio\"], samplerate=speech[\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import SpeechT5Processor, SpeechT5ForTextToSpeech, SpeechT5HifiGan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processor = SpeechT5Processor.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "# model = SpeechT5ForTextToSpeech.from_pretrained(\"microsoft/speecht5_tts\")\n",
    "# vocoder = SpeechT5HifiGan.from_pretrained(\"microsoft/speecht5_hifigan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "# params = sum([np.prod(p.size()) for p in model_parameters])\n",
    "# params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs = processor(text=\"\"\"\n",
    "# Few things I wish I said (I'll add items here as they come up):\n",
    "# - The dreams and hallucinations do not get fixed with finetuning. Finetuning just \"directs\" the dreams into \"helpful assistant dreams\". \n",
    "# \"\"\", return_tensors=\"pt\")\n",
    "# speech = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=vocoder)\n",
    "# sf.write(\"speech.wav\", speech.numpy(), samplerate=16_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyannote.audio.pipelines import VoiceActivityDetection\n",
    "# from pyannote.audio import Model\n",
    "from pyannote.audio import Pipeline\n",
    "from pyannote.core import Segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINGFACE_TOKEN = '...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Model.from_pretrained(\n",
    "#   \"pyannote/segmentation-3.0\", \n",
    "#   use_auth_token=HUGGINGFACE_TOKEN)\n",
    "# pipeline = VoiceActivityDetection(segmentation=model)\n",
    "# HYPER_PARAMETERS = {\n",
    "#   # remove speech regions shorter than that many seconds.\n",
    "#   \"min_duration_on\": 1.0,\n",
    "#   # fill non-speech regions shorter than that many seconds.\n",
    "#   \"min_duration_off\": 1.0\n",
    "# }\n",
    "# pipeline.instantiate(HYPER_PARAMETERS)\n",
    "# vad = pipeline(f'{OUTPUT_PATH}/{FILENAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization-3.1\",\n",
    "                                    use_auth_token=HUGGINGFACE_TOKEN).to(torch.device('cuda'))\n",
    "diarization = pipeline(f'{OUTPUT_PATH}/{FILENAME}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<pyannote.core.annotation.Annotation at 0x1fcc66132d0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_segments = [segment for segment, _, speaker in diarization.itertracks(yield_label=True) if speaker == diarization.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.633276740237692\n",
      "19.906621392190154\n",
      "21.99490662139219\n",
      "25.0\n",
      "27.478777589134125\n",
      "30.07640067911715\n",
      "30.161290322580648\n",
      "31.825127334465193\n",
      "34.30390492359932\n",
      "39.92359932088285\n",
      "46.00169779286927\n",
      "49.77079796264856\n",
      "53.89643463497453\n",
      "60.806451612903224\n",
      "64.54159592529712\n",
      "70.33106960950764\n",
      "76.71477079796264\n",
      "81.74023769100171\n",
      "85.2207130730051\n",
      "88.5823429541596\n",
      "96.29032258064515\n",
      "101.2478777589134\n",
      "105.1358234295416\n",
      "111.12903225806451\n",
      "114.37181663837012\n",
      "116.13752122241087\n",
      "116.37521222410865\n",
      "118.24278438030561\n",
      "121.9100169779287\n",
      "123.77758913412563\n",
      "124.13412563667234\n",
      "128.73514431239389\n",
      "132.3344651952462\n",
      "137.24108658743634\n",
      "146.39219015280136\n",
      "150.36502546689306\n",
      "154.28692699490662\n",
      "156.54499151103568\n",
      "156.86757215619696\n",
      "159.4482173174873\n",
      "159.58404074702887\n",
      "162.7589134125637\n",
      "162.8947368421053\n",
      "164.7113752122241\n",
      "168.34465195246182\n",
      "170.99320882852294\n",
      "171.383701188455\n",
      "173.74363327674024\n",
      "177.05432937181666\n",
      "178.616298811545\n",
      "178.8539898132428\n",
      "181.33276740237693\n",
      "184.76230899830222\n",
      "187.69949066213923\n",
      "187.9881154499151\n",
      "188.8539898132428\n",
      "192.84380305602718\n",
      "196.22241086587437\n",
      "196.46010186757218\n",
      "199.5331069609508\n",
      "211.02716468590833\n",
      "219.70288624787779\n",
      "223.62478777589135\n",
      "226.66383701188457\n",
      "226.98641765704585\n",
      "231.5195246179966\n",
      "235.5772495755518\n",
      "239.46519524617997\n",
      "239.87266553480478\n",
      "245.1528013582343\n",
      "258.3276740237691\n",
      "264.6774193548388\n",
      "268.616298811545\n",
      "275.0339558573854\n",
      "278.83701188455007\n",
      "287.63157894736844\n",
      "291.383701188455\n",
      "292.07979626485576\n",
      "292.38539898132433\n",
      "295.61120543293725\n",
      "295.95076400679113\n",
      "297.2241086587436\n",
      "297.3089983022071\n",
      "299.05772495755525\n"
     ]
    }
   ],
   "source": [
    "for segment in concatenated_segments:\n",
    "    print(segment.start)\n",
    "    print(segment.end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from TTS.tts.configs.xtts_config import XttsConfig\n",
    "# from TTS.tts.models.xtts import Xtts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS.api import TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n",
      " > Using model: xtts\n"
     ]
    }
   ],
   "source": [
    "tts_model = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "[\"Few things I wish I said (I'll add items here as they come up):\", '- The dreams and hallucinations do not get fixed with finetuning.', 'Finetuning just \"directs\" the dreams into \"helpful assistant dreams\".', 'Always be careful with what LLMs tell you, especially if they are telling you something from memory alone.', 'That said, similar to a human, if the LLM used browsing or retrieval and the answer made its way into the \"working memory\" of its context window, you can trust the LLM a bit more to process that information into the final answer.', 'But TLDR right now, do not trust what LLMs say or do.', \"For example, in the tools section, I'd always recommend double-checking the math/code the LLM did.\", '- How does the LLM use a tool like the browser?', 'It emits special words, e.g. |BROWSER|.', 'When the code \"above\" that is inferencing the LLM detects these words it captures the output that follows, sends it off to a tool, comes back with the result and continues the generation.', 'How does the LLM know to emit these special words?', 'Finetuning datasets teach it how and when to browse, by example.', 'And/or the instructions for tool use can also be automatically placed in the context window (in the “system message”).', '- You might also enjoy my 2015 blog post \"Unreasonable Effectiveness of Recurrent Neural Networks\".', 'The way we obtain base models today is pretty much identical on a high level, except the RNN is swapped for a Transformer.', 'http://karpathy.github.io/2015/05/21/...', '- What is in the run.c file?', 'A bit more full-featured 1000-line version hre: https://github.com/karpathy/llama2.c/']\n",
      " > Processing time: 192.05798649787903\n",
      " > Real-time factor: 1.3749748706084717\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'speech.wav'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tts_model.tts_to_file(text=\"\"\"\n",
    "                    Few things I wish I said (I'll add items here as they come up):\n",
    "                    - The dreams and hallucinations do not get fixed with finetuning. Finetuning just \"directs\" the dreams into \"helpful assistant dreams\". Always be careful with what LLMs tell you, especially if they are telling you something from memory alone. That said, similar to a human, if the LLM used browsing or retrieval and the answer made its way into the \"working memory\" of its context window, you can trust the LLM a bit more to process that information into the final answer. But TLDR right now, do not trust what LLMs say or do. For example, in the tools section, I'd always recommend double-checking the math/code the LLM did.\n",
    "                    - How does the LLM use a tool like the browser? It emits special words, e.g. |BROWSER|. When the code \"above\" that is inferencing the LLM detects these words it captures the output that follows, sends it off to a tool, comes back with the result and continues the generation. How does the LLM know to emit these special words? Finetuning datasets teach it how and when to browse, by example. And/or the instructions for tool use can also be automatically placed in the context window (in the “system message”).\n",
    "                    - You might also enjoy my 2015 blog post \"Unreasonable Effectiveness of Recurrent Neural Networks\". The way we obtain base models today is pretty much identical on a high level, except the RNN is swapped for a Transformer. http://karpathy.github.io/2015/05/21/...\n",
    "                    - What is in the run.c file? A bit more full-featured 1000-line version hre: https://github.com/karpathy/llama2.c/\"\"\",\n",
    "                file_path=\"speech.wav\",\n",
    "                speaker_wav=f'{OUTPUT_PATH}/{FILENAME}',\n",
    "                language=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate speech by cloning a voice using default settings\n",
    "# tts.tts_with_vc_to_file(text=\"\"\"\n",
    "#                     Few things I wish I said (I'll add items here as they come up):\n",
    "#                     - The dreams and hallucinations do not get fixed with finetuning. Finetuning just \"directs\" the dreams into \"helpful assistant dreams\". Always be careful with what LLMs tell you, especially if they are telling you something from memory alone. That said, similar to a human, if the LLM used browsing or retrieval and the answer made its way into the \"working memory\" of its context window, you can trust the LLM a bit more to process that information into the final answer. But TLDR right now, do not trust what LLMs say or do. For example, in the tools section, I'd always recommend double-checking the math/code the LLM did.\n",
    "#                     - How does the LLM use a tool like the browser? It emits special words, e.g. |BROWSER|. When the code \"above\" that is inferencing the LLM detects these words it captures the output that follows, sends it off to a tool, comes back with the result and continues the generation. How does the LLM know to emit these special words? Finetuning datasets teach it how and when to browse, by example. And/or the instructions for tool use can also be automatically placed in the context window (in the “system message”).\n",
    "#                     - You might also enjoy my 2015 blog post \"Unreasonable Effectiveness of Recurrent Neural Networks\". The way we obtain base models today is pretty much identical on a high level, except the RNN is swapped for a Transformer. http://karpathy.github.io/2015/05/21/...\n",
    "#                     - What is in the run.c file? A bit more full-featured 1000-line version hre: https://github.com/karpathy/llama2.c/\"\"\",\n",
    "#                 file_path=\"speech.wav\",\n",
    "#                 speaker_wav=f'{OUTPUT_PATH}/{FILENAME}',\n",
    "#                 language=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > voice_conversion_models/multilingual/vctk/freevc24 is already downloaded.\n",
      " > Using model: freevc\n",
      " > Loading pretrained speaker encoder model ...\n",
      "Loaded the voice encoder model on cuda in 0.13 seconds.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 12.08 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 60.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m voice_conv \u001b[38;5;241m=\u001b[39m TTS(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvoice_conversion_models/multilingual/vctk/freevc24\u001b[39m\u001b[38;5;124m\"\u001b[39m, progress_bar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mvoice_conv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvoice_conversion_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_wav\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspeech.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_wav\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mOUTPUT_PATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mFILENAME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspeech2.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\_UNIVER\\UCU\\gen_ai_school\\hack\\.venv\\Lib\\site-packages\\TTS\\api.py:377\u001b[0m, in \u001b[0;36mTTS.voice_conversion_to_file\u001b[1;34m(self, source_wav, target_wav, file_path)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvoice_conversion_to_file\u001b[39m(\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    363\u001b[0m     source_wav: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    364\u001b[0m     target_wav: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    365\u001b[0m     file_path: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    366\u001b[0m ):\n\u001b[0;32m    367\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Voice conversion with FreeVC. Convert source wav to target speaker.\u001b[39;00m\n\u001b[0;32m    368\u001b[0m \n\u001b[0;32m    369\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m            Output file path. Defaults to \"output.wav\".\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 377\u001b[0m     wav \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvoice_conversion\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_wav\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_wav\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_wav\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_wav\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    378\u001b[0m     save_wav(wav\u001b[38;5;241m=\u001b[39mwav, path\u001b[38;5;241m=\u001b[39mfile_path, sample_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvoice_converter\u001b[38;5;241m.\u001b[39mvc_config\u001b[38;5;241m.\u001b[39maudio\u001b[38;5;241m.\u001b[39moutput_sample_rate)\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file_path\n",
      "File \u001b[1;32me:\\_UNIVER\\UCU\\gen_ai_school\\hack\\.venv\\Lib\\site-packages\\TTS\\api.py:358\u001b[0m, in \u001b[0;36mTTS.voice_conversion\u001b[1;34m(self, source_wav, target_wav)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvoice_conversion\u001b[39m(\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    347\u001b[0m     source_wav: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    348\u001b[0m     target_wav: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    349\u001b[0m ):\n\u001b[0;32m    350\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Voice conversion with FreeVC. Convert source wav to target speaker.\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m    Args:``\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;124;03m            Path to the target wav file.\u001b[39;00m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 358\u001b[0m     wav \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvoice_converter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvoice_conversion\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_wav\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_wav\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_wav\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_wav\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wav\n",
      "File \u001b[1;32me:\\_UNIVER\\UCU\\gen_ai_school\\hack\\.venv\\Lib\\site-packages\\TTS\\utils\\synthesizer.py:254\u001b[0m, in \u001b[0;36mSynthesizer.voice_conversion\u001b[1;34m(self, source_wav, target_wav)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvoice_conversion\u001b[39m(\u001b[38;5;28mself\u001b[39m, source_wav: \u001b[38;5;28mstr\u001b[39m, target_wav: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m--> 254\u001b[0m     output_wav \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvc_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvoice_conversion\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_wav\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_wav\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output_wav\n",
      "File \u001b[1;32me:\\_UNIVER\\UCU\\gen_ai_school\\hack\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\_UNIVER\\UCU\\gen_ai_school\\hack\\.venv\\Lib\\site-packages\\TTS\\vc\\models\\freevc.py:522\u001b[0m, in \u001b[0;36mFreeVC.voice_conversion\u001b[1;34m(self, src, tgt)\u001b[0m\n\u001b[0;32m    519\u001b[0m wav_tgt, _ \u001b[38;5;241m=\u001b[39m librosa\u001b[38;5;241m.\u001b[39meffects\u001b[38;5;241m.\u001b[39mtrim(wav_tgt, top_db\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m    521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmodel_args\u001b[38;5;241m.\u001b[39muse_spk:\n\u001b[1;32m--> 522\u001b[0m     g_tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menc_spk_ex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_utterance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwav_tgt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    523\u001b[0m     g_tgt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(g_tgt)[\u001b[38;5;28;01mNone\u001b[39;00m, :, \u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    524\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32me:\\_UNIVER\\UCU\\gen_ai_school\\hack\\.venv\\Lib\\site-packages\\TTS\\vc\\modules\\freevc\\speaker_encoder\\speaker_encoder.py:155\u001b[0m, in \u001b[0;36mSpeakerEncoder.embed_utterance\u001b[1;34m(self, wav, return_partials, rate, min_coverage)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m    154\u001b[0m     mels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(mels)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 155\u001b[0m     partial_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    157\u001b[0m \u001b[38;5;66;03m# Compute the utterance embedding from the partial embeddings\u001b[39;00m\n\u001b[0;32m    158\u001b[0m raw_embed \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(partial_embeds, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32me:\\_UNIVER\\UCU\\gen_ai_school\\hack\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\_UNIVER\\UCU\\gen_ai_school\\hack\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\_UNIVER\\UCU\\gen_ai_school\\hack\\.venv\\Lib\\site-packages\\TTS\\vc\\modules\\freevc\\speaker_encoder\\speaker_encoder.py:60\u001b[0m, in \u001b[0;36mSpeakerEncoder.forward\u001b[1;34m(self, mels)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03mComputes the embeddings of a batch of utterance spectrograms.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03m:param mels: a batch of mel spectrograms of same duration as a float32 tensor of shape\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03mEmbeddings are positive and L2-normed, thus they lay in the range [0, 1].\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Pass the input through the LSTM layers and retrieve the final hidden state of the last\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# layer. Apply a cutoff to 0 for negative values and L2 normalize the embeddings.\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m _, (hidden, _) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m embeds_raw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(hidden[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeds_raw \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39mnorm(embeds_raw, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32me:\\_UNIVER\\UCU\\gen_ai_school\\hack\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\_UNIVER\\UCU\\gen_ai_school\\hack\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\_UNIVER\\UCU\\gen_ai_school\\hack\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:878\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    875\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 878\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[0;32m    882\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 12.08 GiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 7.06 GiB is allocated by PyTorch, and 60.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "voice_conv = TTS(model_name=\"voice_conversion_models/multilingual/vctk/freevc24\", progress_bar=True).to(\"cuda\")\n",
    "voice_conv.voice_conversion_to_file(source_wav=\"speech.wav\", target_wav=f'{OUTPUT_PATH}/{FILENAME}', file_path=\"speech2.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'voice_conversion'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtts_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvoice_conversion_to_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_wav\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspeech.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_wav\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mOUTPUT_PATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mFILENAME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspeech3.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\_UNIVER\\UCU\\gen_ai_school\\hack\\.venv\\Lib\\site-packages\\TTS\\api.py:377\u001b[0m, in \u001b[0;36mTTS.voice_conversion_to_file\u001b[1;34m(self, source_wav, target_wav, file_path)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvoice_conversion_to_file\u001b[39m(\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    363\u001b[0m     source_wav: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    364\u001b[0m     target_wav: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    365\u001b[0m     file_path: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    366\u001b[0m ):\n\u001b[0;32m    367\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Voice conversion with FreeVC. Convert source wav to target speaker.\u001b[39;00m\n\u001b[0;32m    368\u001b[0m \n\u001b[0;32m    369\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m            Output file path. Defaults to \"output.wav\".\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 377\u001b[0m     wav \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvoice_conversion\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_wav\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource_wav\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_wav\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_wav\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    378\u001b[0m     save_wav(wav\u001b[38;5;241m=\u001b[39mwav, path\u001b[38;5;241m=\u001b[39mfile_path, sample_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvoice_converter\u001b[38;5;241m.\u001b[39mvc_config\u001b[38;5;241m.\u001b[39maudio\u001b[38;5;241m.\u001b[39moutput_sample_rate)\n\u001b[0;32m    379\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file_path\n",
      "File \u001b[1;32me:\\_UNIVER\\UCU\\gen_ai_school\\hack\\.venv\\Lib\\site-packages\\TTS\\api.py:358\u001b[0m, in \u001b[0;36mTTS.voice_conversion\u001b[1;34m(self, source_wav, target_wav)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvoice_conversion\u001b[39m(\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    347\u001b[0m     source_wav: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    348\u001b[0m     target_wav: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    349\u001b[0m ):\n\u001b[0;32m    350\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Voice conversion with FreeVC. Convert source wav to target speaker.\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \n\u001b[0;32m    352\u001b[0m \u001b[38;5;124;03m    Args:``\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;124;03m            Path to the target wav file.\u001b[39;00m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 358\u001b[0m     wav \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvoice_converter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvoice_conversion\u001b[49m(source_wav\u001b[38;5;241m=\u001b[39msource_wav, target_wav\u001b[38;5;241m=\u001b[39mtarget_wav)\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m wav\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'voice_conversion'"
     ]
    }
   ],
   "source": [
    "tts_model.voice_conversion_to_file(source_wav=\"speech.wav\", target_wav=f'{OUTPUT_PATH}/{FILENAME}', file_path=\"speech3.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/en/vctk/vits is already downloaded.\n",
      " > Using model: vits\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log10\n",
      " | > min_level_db:0\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:None\n",
      " | > fft_size:1024\n",
      " | > power:None\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:None\n",
      " | > signal_norm:None\n",
      " | > symmetric_norm:None\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:None\n",
      " | > pitch_fmin:None\n",
      " | > pitch_fmax:None\n",
      " | > spec_gain:20.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:1.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:10\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > initialization of speaker-embedding layers.\n"
     ]
    }
   ],
   "source": [
    "tts = TTS(\"tts_models/en/vctk/vits\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts = TTS(model_name=\"voice_conversion_models/multilingual/vctk/freevc24\", progress_bar=False).to(\"cuda\")\n",
    "tts.voice_conversion_to_file(source_wav=\"my/source.wav\", target_wav=\"my/target.wav\", file_path=\"output.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "[\"Few things I wish I said (I'll add items here as they come up):\", '- The dreams and hallucinations do not get fixed with finetuning.', 'Finetuning just \"directs\" the dreams into \"helpful assistant dreams\".', 'Always be careful with what LLMs tell you, especially if they are telling you something from memory alone.', 'That said, similar to a human, if the LLM used browsing or retrieval and the answer made its way into the \"working memory\" of its context window, you can trust the LLM a bit more to process that information into the final answer.', 'But TLDR right now, do not trust what LLMs say or do.', \"For example, in the tools section, I'd always recommend double-checking the math/code the LLM did.\", '- How does the LLM use a tool like the browser?', 'It emits special words, e.g. |BROWSER|.', 'When the code \"above\" that is inferencing the LLM detects these words it captures the output that follows, sends it off to a tool, comes back with the result and continues the generation.', 'How does the LLM know to emit these special words?', 'Finetuning datasets teach it how and when to browse, by example.', 'And/or the instructions for tool use can also be automatically placed in the context window (in the “system message”).', '- You might also enjoy my 2015 blog post \"Unreasonable Effectiveness of Recurrent Neural Networks\".', 'The way we obtain base models today is pretty much identical on a high level, except the RNN is swapped for a Transformer.', 'http://karpathy.github.io/2015/05/21/...', '- What is in the run.c file?', 'A bit more full-featured 1000-line version hre: https://github.com/karpathy/llama2.c/']\n",
      " > Processing time: 15.926865339279175\n",
      " > Real-time factor: 0.14491945817120333\n",
      " > voice_conversion_models/multilingual/vctk/freevc24 is already downloaded.\n",
      " > Using model: freevc\n",
      " > Loading pretrained speaker encoder model ...\n",
      "Loaded the voice encoder model on cuda in 0.80 seconds.\n"
     ]
    }
   ],
   "source": [
    "tts.tts_with_vc_to_file(\n",
    "    \"\"\"\n",
    "        Few things I wish I said (I'll add items here as they come up):\n",
    "        - The dreams and hallucinations do not get fixed with finetuning. Finetuning just \"directs\" the dreams into \"helpful assistant dreams\". Always be careful with what LLMs tell you, especially if they are telling you something from memory alone. That said, similar to a human, if the LLM used browsing or retrieval and the answer made its way into the \"working memory\" of its context window, you can trust the LLM a bit more to process that information into the final answer. But TLDR right now, do not trust what LLMs say or do. For example, in the tools section, I'd always recommend double-checking the math/code the LLM did.\n",
    "        - How does the LLM use a tool like the browser? It emits special words, e.g. |BROWSER|. When the code \"above\" that is inferencing the LLM detects these words it captures the output that follows, sends it off to a tool, comes back with the result and continues the generation. How does the LLM know to emit these special words? Finetuning datasets teach it how and when to browse, by example. And/or the instructions for tool use can also be automatically placed in the context window (in the “system message”).\n",
    "        - You might also enjoy my 2015 blog post \"Unreasonable Effectiveness of Recurrent Neural Networks\". The way we obtain base models today is pretty much identical on a high level, except the RNN is swapped for a Transformer. http://karpathy.github.io/2015/05/21/...\n",
    "        - What is in the run.c file? A bit more full-featured 1000-line version hre: https://github.com/karpathy/llama2.c/\"\"\",\n",
    "    file_path=\"speech2.wav\",\n",
    "    speaker_wav=f'{OUTPUT_PATH}/{FILENAME}',\n",
    "    # language='en',\n",
    "    speaker='p225',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # generate speech by cloning a voice using default settings\n",
    "# tts.tts_to_file(text=\"\"\"                    \n",
    "#                     Few things I wish I said (I'll add items here as they come up):\n",
    "#                     - The dreams and hallucinations do not get fixed with finetuning. Finetuning just \"directs\" the dreams into \"helpful assistant dreams\". Always be careful with what LLMs tell you, especially if they are telling you something from memory alone. That said, similar to a human, if the LLM used browsing or retrieval and the answer made its way into the \"working memory\" of its context window, you can trust the LLM a bit more to process that information into the final answer. But TLDR right now, do not trust what LLMs say or do. For example, in the tools section, I'd always recommend double-checking the math/code the LLM did.\n",
    "#                     - How does the LLM use a tool like the browser? It emits special words, e.g. |BROWSER|. When the code \"above\" that is inferencing the LLM detects these words it captures the output that follows, sends it off to a tool, comes back with the result and continues the generation. How does the LLM know to emit these special words? Finetuning datasets teach it how and when to browse, by example. And/or the instructions for tool use can also be automatically placed in the context window (in the “system message”).\n",
    "#                     - You might also enjoy my 2015 blog post \"Unreasonable Effectiveness of Recurrent Neural Networks\". The way we obtain base models today is pretty much identical on a high level, except the RNN is swapped for a Transformer. http://karpathy.github.io/2015/05/21/...\n",
    "#                     - What is in the run.c file? A bit more full-featured 1000-line version hre: https://github.com/karpathy/llama2.c/\"\"\",\n",
    "#                 file_path=\"speech.wav\",\n",
    "#                 speaker_wav=f'{OUTPUT_PATH}/{FILENAME}',)\n",
    "#                 # language=\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = XttsConfig()\n",
    "# config.load_json(\"/path/to/xtts/config.json\")\n",
    "# model = Xtts.init_from_config(config)\n",
    "# model.load_checkpoint(config, checkpoint_dir=\"/path/to/xtts/\", eval=True)\n",
    "# model.cuda()\n",
    "\n",
    "# outputs = model.synthesize(\n",
    "#     \"It took me quite a long time to develop a voice and now that I have it I am not going to be silent.\",\n",
    "#     config,\n",
    "#     speaker_wav=\"/data/TTS-public/_refclips/3.wav\",\n",
    "#     gpt_cond_len=3,\n",
    "#     language=\"en\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
